{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Classification - PyTorch\n",
    "updated: Sep. 01, 2018\n",
    "\n",
    "Data: https://www.physionet.org/pn4/eegmmidb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# System\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "from io import StringIO\n",
    "import shutil\n",
    "import pathlib\n",
    "import urllib\n",
    "\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "# Essential Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil, floor\n",
    "\n",
    "# Get Paths\n",
    "from glob import glob\n",
    "\n",
    "# EEG package\n",
    "from mne import pick_types, events_from_annotations\n",
    "from mne.io import read_raw_edf\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# PyThorch \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# To use Tensorflow with PyTorch\n",
    "from torchbearer import Trial\n",
    "from torchbearer.callbacks import TensorBoard, Best, ReduceLROnPlateau, CSVLogger, ModelCheckpoint\n",
    "\n",
    "# To parse input arguments\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "Subjects performed different motor/imagery tasks while 64-channel EEG were recorded using the BCI2000 system (http://www.bci2000.org). Each subject performed 14 experimental runs: \n",
    "\n",
    "- two one-minute baseline runs (one with eyes open, one with eyes closed)\n",
    "- three two-minute runs of each of the four following tasks:\n",
    "    - 1:\n",
    "        - A target appears on either the left or the right side of the screen. \n",
    "        - The subject opens and closes the corresponding fist until the target disappears. \n",
    "        - Then the subject relaxes.\n",
    "    - 2:\n",
    "        - A target appears on either the left or the right side of the screen. \n",
    "        - The subject imagines opening and closing the corresponding fist until the target disappears. \n",
    "        - Then the subject relaxes.\n",
    "    - 3:\n",
    "        - A target appears on either the top or the bottom of the screen. \n",
    "        - The subject opens and closes either both fists (if the target is on top) or both feet (if the target is on the bottom) until the target disappears. \n",
    "        - Then the subject relaxes.\n",
    "    - 4:\n",
    "        - A target appears on either the top or the bottom of the screen. \n",
    "        - The subject imagines opening and closing either both fists (if the target is on top) or both feet (if the target is on the bottom) until the target disappears. \n",
    "        - Then the subject relaxes.\n",
    "\n",
    "The data are provided here in EDF+ format (containing 64 EEG signals, each sampled at 160 samples per second, and an annotation channel). \n",
    "For use with PhysioToolkit software, rdedfann generated a separate PhysioBank-compatible annotation file (with the suffix .event) for each recording. \n",
    "The .event files and the annotation channels in the corresponding .edf files contain identical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary tasks\n",
    "\n",
    "Remembering that:\n",
    "\n",
    "    - Task 1 (open and close left or right fist)\n",
    "    - Task 2 (imagine opening and closing left or right fist)\n",
    "    - Task 3 (open and close both fists or both feet)\n",
    "    - Task 4 (imagine opening and closing both fists or both feet)\n",
    "\n",
    "we will referred to 'Task *' with the meneaning above. \n",
    "\n",
    "In summary, the experimental runs were:\n",
    "\n",
    "1.  Baseline, eyes open\n",
    "2.  Baseline, eyes closed\n",
    "3.  Task 1 \n",
    "4.  Task -2 \n",
    "5.  Task --3 \n",
    "6.  Task ---4 \n",
    "7.  Task 1\n",
    "8.  Task -2\n",
    "9.  Task --3\n",
    "10. Task ---4\n",
    "11. Task 1\n",
    "12. Task -2\n",
    "13. Task --3\n",
    "14. Task ---4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation\n",
    "\n",
    "Each annotation includes one of three codes (T0, T1, or T2):\n",
    "\n",
    "- T0 corresponds to rest\n",
    "- T1 corresponds to onset of motion (real or imagined) of\n",
    "    - the left fist (in runs 3, 4, 7, 8, 11, and 12)\n",
    "    - both fists (in runs 5, 6, 9, 10, 13, and 14)\n",
    "- T2 corresponds to onset of motion (real or imagined) of\n",
    "    - the right fist (in runs 3, 4, 7, 8, 11, and 12)\n",
    "    - both feet (in runs 5, 6, 9, 10, 13, and 14)\n",
    "    \n",
    "In the BCI2000-format versions of these files, which may be available from the contributors of this data set, these annotations are encoded as values of 0, 1, or 2 in the TargetCode state variable.\n",
    "\n",
    "{'T0':0, 'T1':1, 'T2':2}\n",
    "\n",
    "In our experiments we will see only :\n",
    "\n",
    "- run_type_0:\n",
    "    - append_X\n",
    "- run_type_1\n",
    "    - append_X_y\n",
    "- run_type_2\n",
    "    - append_X_y\n",
    "    \n",
    "and the coding is: \n",
    "\n",
    "- T0 corresponds to rest \n",
    "    - (2)\n",
    "- T1 (real or imagined)\n",
    "    - (4,  8, 12) the left fist \n",
    "    - (6, 10, 14) both fists \n",
    "- T2 (real or imagined)\n",
    "    - (4,  8, 12) the right fist \n",
    "    - (6, 10, 14) both feet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling - Time-Distributed CNN + RNN\n",
    "\n",
    "Training Plan:\n",
    "\n",
    "+ 4 GPU units (Nvidia Tesla P100) were used to train this neural network.\n",
    "+ Instead of training the whole model at once, I trained the first block (CNN) first. Then using the trained parameters as initial values, I trained the next blocks step-by-step. This approach can greatly reduce the time required for training and help avoiding falling into local minimums.\n",
    "+ The first blocks (CNN) can be applied for other EEG classification models as a pre-trained base.\n",
    "\n",
    "+ The initial learning rate is set to be $10^{3}$ with Adam optimization. I used several callbacks such as ReduceLROnPlateau which adjusts the learning rate at local minima. Also, I record the log for tensorboard to monitor the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Pytorch Implementation\n",
    "\n",
    "Based on MNIST CNN + LSTM example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "# num_input = X_train.shape[0]   # PhysioNet data input (mesh shape: 10*11)\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.test_rate  = 0.2\n",
    "        self.run_number = 0\n",
    "        self.cuda       = False\n",
    "        self.no_cuda    = True\n",
    "        self.seed       = 42\n",
    "        self.batch_size = 128\n",
    "        self.val_batch_size  = 128\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs         = 10\n",
    "        self.lr             = 0.001\n",
    "        self.momentum       = 0.5\n",
    "        self.log_interval   = 10\n",
    "        self.n_nodes        = [3,2,2]# . structure of network (e.s. [3,2,2]: 3 CNN + 2 LSTM + 2 FC )\n",
    "        self.cnn_filter     = 32     # . number filter for the first layer of CNN\n",
    "        self.cnn_kernelsize = 3      # . dimension of kernel for CNN layer\n",
    "        self.lstm_hidden    = 64     # . number of elements for hidden state of lstm\n",
    "        self.fc_dim         = 1024   # . number of neurons for fully connected layer\n",
    "        self.fc_dropout     = 0.5    # . drop out rate for fully connected layer\n",
    "        self.rcnn_output    = 5      # . output of network\n",
    "        self.num_classes    = 5      # PhysioNet total classes\n",
    "        self.split_type = 'user_dependent' \n",
    "     \n",
    "\n",
    "args = Args()\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_train, y_train, p_train] = pickle.load( open( \"./dataset/splitted_data/\"+args.split_type+\"/test_rate_\"+str(args.test_rate)+\"/seed_\"+str(args.seed)+\"/train.p\", \"rb\" ) )\n",
    "[X_val, y_val, p_val]       = pickle.load( open( \"./dataset/splitted_data/\"+args.split_type+\"/test_rate_\"+str(args.test_rate)+\"/seed_\"+str(args.seed)+\"/val.p\"  , \"rb\" ) )\n",
    "[X_test, y_test, p_test]    = pickle.load( open( \"./dataset/splitted_data/\"+args.split_type+\"/test_rate_\"+str(args.test_rate)+\"/seed_\"+str(args.seed)+\"/test.p\" , \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train X (316593, 10, 10, 11, 1) y (316593, 1)\n",
      "val   X (79149, 10, 10, 11, 1) y (79149, 1)\n",
      "test  X (123670, 10, 10, 11, 1) y (123670, 1)\n"
     ]
    }
   ],
   "source": [
    "print('train', 'X', X_train.shape, 'y', y_train.shape) #, 'p', p_train.shape)\n",
    "print('val  ', 'X', X_val.shape  , 'y', y_val.shape) #, 'p', p_val.shape)\n",
    "print('test ', 'X', X_test.shape , 'y', y_test.shape) #, 'p', p_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "squeeze of all X, y - train, val, test\n",
      "\n",
      "train X (316593, 10, 10, 11) y (316593,)\n",
      "val   X (79149, 10, 10, 11) y (79149,)\n",
      "test  X (123670, 10, 10, 11) y (123670,)\n"
     ]
    }
   ],
   "source": [
    "print('\\nsqueeze of all X, y - train, val, test\\n')\n",
    "X_train = X_train.squeeze()\n",
    "y_train = y_train.squeeze()\n",
    "X_val = X_val.squeeze()\n",
    "y_val = y_val.squeeze()\n",
    "X_test = X_test.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "print('train', 'X', X_train.shape, 'y', y_train.shape) #, 'p', p_train.shape)\n",
    "print('val  ', 'X', X_val.shape  , 'y', y_val.shape) #, 'p', p_val.shape)\n",
    "print('test ', 'X', X_test.shape , 'y', y_test.shape) #, 'p', p_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGImagesDatasetLoader(Dataset):\n",
    "    \"\"\"EEGs (converted in images) dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, pickle_file='train.p', root_dir='./py/stack/', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        [self.X, self.y] = pickle.load( open( root_dir + pickle_file, \"rb\" ) )\n",
    "        \n",
    "        '''\n",
    "        def one_hot_embedding(labels, num_classes=args.num_classes):\n",
    "            \"\"\"Embedding labels to one-hot form.\n",
    "\n",
    "            Args:\n",
    "              labels: (LongTensor) class labels, sized [N,].\n",
    "              num_classes: (int) number of classes.\n",
    "\n",
    "            Returns:\n",
    "              (tensor) encoded labels, sized [N, #classes].\n",
    "            \"\"\"\n",
    "            y = torch.eye(num_classes) \n",
    "            return y[labels] \n",
    "        '''\n",
    "\n",
    "        # self.y = one_hot_embedding(self.y, args.num_classes )\n",
    "        # print(self.y)\n",
    "        self.y = torch.tensor(self.y, dtype=torch.long)\n",
    "        # print(self.y.size())\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            # If the transform variable is not empty\n",
    "            # then it applies the operations in the transforms with the order that it is created.\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGImagesDatasetHolding(Dataset):\n",
    "    \"\"\"EEGs (converted in images) dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X, self.y = X, y\n",
    "        \n",
    "        '''\n",
    "        def one_hot_embedding(labels, num_classes=args.num_classes):\n",
    "            \"\"\"Embedding labels to one-hot form.\n",
    "\n",
    "            Args:\n",
    "              labels: (LongTensor) class labels, sized [N,].\n",
    "              num_classes: (int) number of classes.\n",
    "\n",
    "            Returns:\n",
    "              (tensor) encoded labels, sized [N, #classes].\n",
    "            \"\"\"\n",
    "            y = torch.eye(num_classes) \n",
    "            return y[labels] \n",
    "        '''\n",
    "\n",
    "        # self.y = one_hot_embedding(self.y, args.num_classes )\n",
    "        # print(self.y)\n",
    "        self.y = torch.tensor(self.y, dtype=torch.long)\n",
    "        # print(self.y.size())\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            # If the transform variable is not empty\n",
    "            # then it applies the operations in the transforms with the order that it is created.\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "eeg_train = EEGImagesDatasetLoader(pickle_file='train_4.p', root_dir='./py/stack/', transform=None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "eeg_test  = EEGImagesDatasetLoader(pickle_file='test_4.p', root_dir='./py/stack/', transform=None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307, ), (0.3081, ))\n",
    "        ]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_train = EEGImagesDatasetHolding(X_train, y_train, transform=None)\n",
    "eeg_val   = EEGImagesDatasetHolding(X_val  , y_val  , transform=None)\n",
    "eeg_test  = EEGImagesDatasetHolding(X_test , y_test , transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    eeg_train,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    **kwargs)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    eeg_val,\n",
    "    batch_size=args.val_batch_size,\n",
    "    shuffle=True,\n",
    "    **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    eeg_test,\n",
    "    batch_size=args.test_batch_size,\n",
    "    shuffle=True,\n",
    "    **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dimension: 316593 (10, 10, 11) torch.Size([])\n",
      "train dimension: 79149 (10, 10, 11) torch.Size([])\n",
      "test  dimension: 123670 (10, 10, 11) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "(image, label) = eeg_train[0]\n",
    "print('train dimension:',eeg_train.__len__(), image.shape, label.shape)\n",
    "\n",
    "(image, label) = eeg_val[0]\n",
    "print('val   dimension:',eeg_val.__len__(), image.shape, label.shape)\n",
    "\n",
    "(image, label) = eeg_test[0]\n",
    "print('test  dimension:',eeg_test.__len__(), image.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1  = nn.Conv2d(10, args.cnn_filter, kernel_size=args.cnn_kernelsize)\n",
    "        self.batch1 = nn.BatchNorm2d(args.cnn_filter)\n",
    "        # self.act1   = F.elu(args.cnn_filter, alpha=1.0, inplace=False)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(args.cnn_filter*1, args.cnn_filter*2, kernel_size=args.cnn_kernelsize)\n",
    "        self.batch2 = nn.BatchNorm2d(args.cnn_filter*2**1)\n",
    "        # self.act2   = F.elu(args.cnn_filter*2**1, alpha=1.0, inplace=False)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d( args.cnn_filter * 2, args.cnn_filter * 2**2, kernel_size=args.cnn_kernelsize)\n",
    "        self.batch3 = nn.BatchNorm2d(args.cnn_filter*2**2)\n",
    "        # self.act3   = F.elu(args.cnn_filter*2**2, alpha=1.0, inplace=False)\n",
    "        \n",
    "        self.FC1 = nn.Linear(2560, args.fc_dim)\n",
    "        self.dropout1 = nn.Dropout2d(p=args.fc_dropout)\n",
    "        self.batch4 = nn.BatchNorm1d(args.fc_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('cnn i', x.shape)\n",
    "        x = F.elu(self.batch1(self.conv1(x)))\n",
    "        # print('cnn 1', x.shape)\n",
    "        x = F.elu(self.batch2(self.conv2(x)))\n",
    "        # print('cnn 2', x.shape)\n",
    "        x = F.elu(self.batch3(self.conv3(x)))\n",
    "        # print('cnn 3', x.shape)\n",
    "        x = x.view(-1, 2560) # 2560 = args.batch_size * x.shape[-1] * x.shape[-2]\n",
    "        # print('cnn o/fc1 in', x.shape)\n",
    "        x = F.elu(self.batch4(self.dropout1(self.FC1(x))))\n",
    "        # print('fc1 o', x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.cnn = CNN() \n",
    "        self.rnn = nn.LSTM(\n",
    "                input_size  = args.fc_dim, \n",
    "                hidden_size = args.lstm_hidden, \n",
    "                num_layers  = 2, \n",
    "                batch_first = True )\n",
    "        self.FC2 = nn.Linear(args.lstm_hidden, args.fc_dim)\n",
    "        self.dropout2 = nn.Dropout(p=args.fc_dropout)\n",
    "        self.FC3 = nn.Linear(args.fc_dim, args.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('combine i', x.size())\n",
    "        batch_size, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size , C, H, W)\n",
    "        # print('combine cin', c_in.size())\n",
    "        c_out = self.cnn(c_in)\n",
    "        # print('combine cout/rin', c_out.size())\n",
    "        r_in = c_out.view(batch_size, 1, -1)\n",
    "        # print('combine rin', r_in.size())\n",
    "        r_out, (h_n, h_c) = self.rnn(r_in)\n",
    "        # print('combine rout', r_out.size(), '(h_n, h_c)', (h_n.size(), h_c.size()))\n",
    "        r_out2 = self.FC3(self.dropout2(self.FC2(r_out[:, -1, :])))\n",
    "        # print('combine o', r_out2.size())\n",
    "        return r_out2 # F.log_softmax(r_out2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "run_to_path = str(args.run_number)+str(args.n_nodes)+'_'+str(current_time)\n",
    "\n",
    "my_log = './torchbearer/log_dir/'\n",
    "my_log_dir = my_log +'CNN_LSTM_run'+run_to_path+'/'\n",
    "if os.path.exists(os.path.dirname(my_log_dir)):\n",
    "    for file in os.listdir(my_log_dir):\n",
    "        file_path = os.path.join(my_log_dir, file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "else:\n",
    "    os.makedirs(os.path.dirname(my_log_dir))\n",
    "\n",
    "save_model_dir = './torchbearer/save_model/CNN_LSTM_run'+run_to_path+'/'\n",
    "if os.path.exists(os.path.dirname(save_model_dir)):\n",
    "    for file in os.listdir(save_model_dir):\n",
    "        file_path = os.path.join(save_model_dir, file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "else:\n",
    "    os.makedirs(os.path.dirname(save_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_report(model, history):\n",
    "    infopath = save_model_dir + \"/info.txt\"\n",
    "    with open(infopath, 'w') as fh:\n",
    "        fh.write(\"Training parameters : \\n\")\n",
    "        fh.write(\"Input dimensions (train) :  X \" + str(X_train.shape) + \", y \" + str(X_train.shape) + \"\\n\")\n",
    "        fh.write(\"Input dimensions (val  ) :  X \" + str(X_val.shape)   + \", y \" + str(X_val.shape)   + \"\\n\")\n",
    "        fh.write(\"Input dimensions (test ) :  X \" + str(X_test.shape)  + \", y \" + str(X_test.shape)  + \"\\n\")\n",
    "        fh.write(\"Epochs - LR - Dropout - Momentum : \" + str(args.epochs) + \" - \" + str(args.lr) + \" - \" + str(args.fc_dropout) + \"-\" + str(args.momentum) + \"\\n\")\n",
    "        fh.write(\"Batch_size - Steps_train - Steps_valid : \" + str(args.batch_size) + \" - \" + str(args.val_batch_size) + \" - \" + str(args.test_batch_size) +\"\\n\")\n",
    "        fh.write(\"Final loss - val_loss : \" + str(min([ history[i]['loss'] for i in range(len(history))])) + \" - \" + str(min([ history[i]['val_loss'] for i in range(len(history))])) + \"\\n\")\n",
    "        fh.write(\"Network architecture : \\n\")\n",
    "        # string = model.state_dict() \n",
    "        # Pass the file handle in as a lambda function to make it callable\n",
    "        # model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "        print( model, file=fh)\n",
    "        \n",
    "\n",
    "callbacks_list = [\n",
    "                # Best(save_model_dir + 'model_EP{epoch:02d}_VA{val_acc:.4f}.pt', save_model_params_only=True), # monitor='val_acc', mode='max'),\n",
    "                # ExponentialLR(gamma=0.1),\n",
    "                # TensorBoardText(comment=current_time),\n",
    "                ModelCheckpoint(save_model_dir + 'model_[epo]{epoch:02d}_[val]{val_acc:.4f}.pt', save_best_only=True, monitor='val_loss'),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5),\n",
    "                CSVLogger(my_log_dir + \"log.csv\", separator=',', append=True),\n",
    "                TensorBoard(my_log, write_graph=True, write_batch_metrics=False, write_epoch_metrics=False, comment='run'+run_to_path),\n",
    "                TensorBoard(my_log, write_graph=False, write_batch_metrics=True, batch_step_size=10, write_epoch_metrics=False, comment='run'+run_to_path),\n",
    "                TensorBoard(my_log, write_graph=False, write_batch_metrics=False, write_epoch_metrics=True, comment='run'+run_to_path)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_LSTM()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "old_stdout = sys.stdout\n",
    "sys.stdout = mystdout = StringIO()\n",
    "os.system('cat /proc/sys/kernel/hostname')\n",
    "sys.stdout = old_stdout\n",
    "\n",
    "d = '######################           '+ ( mystdout.getvalue()+'\\n' if os.system('uname') == 'Linux' else 'MAC\\n' )\n",
    "\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = mystdout = StringIO()\n",
    "print( model )\n",
    "sys.stdout = old_stdout\n",
    "d += '######################           Network Architectures'+mystdout.getvalue()\n",
    "os.system(\"curl -X POST -H 'Content-type: application/json' --data '{\\\"text\\\":\\\"\"+str(d)+\"\\n\\\"}' https://hooks.slack.com/services/mysercret\")\n",
    "\n",
    "print('\\n# ==================================================== #')\n",
    "print('#                        Model                         #')\n",
    "print('# ==================================================== #\\n\\n',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44691daea654f5586b3a72ec934f340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='0/1(t)', max=79, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c476986a104ef890051995b0bae7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='0/1(v)', max=8, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'running_acc': 0.21046875417232513, 'running_loss': 1.6096813678741455, 'acc': 0.20856408774852753, 'loss': 1.6097244024276733, 'val_acc': 0.216796875, 'val_loss': 1.6094927787780762, 'train_steps': 79, 'validation_steps': 8}]\n"
     ]
    }
   ],
   "source": [
    "trial = Trial(model, optimizer, loss, metrics=['acc', 'loss'], callbacks=callbacks_list) #.to('cuda')\n",
    "history = trial.with_generators(train_generator=train_loader, val_generator=val_loader, test_generator=test_loader, train_steps=79, val_steps=8, test_steps=8).run(args.epochs)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################           Network History\n",
      " ####    EPOCH 0\n",
      "running_acc : 0.20125000178813934\n",
      "running_loss : 1.6104434728622437\n",
      "acc : 0.19740000367164612\n",
      "loss : 1.6109141111373901\n",
      "val_acc : 0.21199999749660492\n",
      "val_loss : 1.6085139513015747\n",
      "train_steps : 79\n",
      "validation_steps : 8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_stdout = sys.stdout\n",
    "sys.stdout = mystdout = StringIO()\n",
    "os.system('cat /proc/sys/kernel/hostname')\n",
    "sys.stdout = old_stdout\n",
    "\n",
    "d = '######################           '+mystdout.getvalue()\n",
    "d = '######################           Network History'\n",
    "for i, epoch in enumerate(history):\n",
    "    d += '\\n ####    EPOCH '+str(i)\n",
    "    for k, v in epoch.items():\n",
    "        d += '\\n'+str(k)+' : '+str(v)\n",
    "    d += '\\n\\n'\n",
    "print(d)\n",
    "os.system(\"curl -X POST -H 'Content-type: application/json' --data '{\\\"text\\\":\\\" \"+d+\"\\\"}' https://hooks.slack.com/services/mysercret\")\n",
    "\n",
    "write_report(model, history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
