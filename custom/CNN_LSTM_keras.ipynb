{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Classification - Tensorflow\n",
    "updated: Sep. 01, 2018\n",
    "\n",
    "Data: https://www.physionet.org/pn4/eegmmidb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Downloads\n",
    "\n",
    "### Warning: Executing these blocks will automatically create directories and download datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:1',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:2',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:3']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow Style Guide\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# System\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import urllib\n",
    "\n",
    "# Modeling & Preprocessing\n",
    "from keras.layers import Conv2D, BatchNormalization, Activation, Flatten, Dense, Dropout, LSTM, Input, TimeDistributed\n",
    "from keras import initializers, Model, optimizers, callbacks\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback, TensorBoard\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "# Essential Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil, floor\n",
    "\n",
    "# Get Paths\n",
    "from glob import glob\n",
    "\n",
    "# EEG package\n",
    "from mne import pick_types, events_from_annotations\n",
    "from mne.io import read_raw_edf\n",
    "\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "Subjects performed different motor/imagery tasks while 64-channel EEG were recorded using the BCI2000 system (http://www.bci2000.org). Each subject performed 14 experimental runs: \n",
    "\n",
    "- two one-minute baseline runs (one with eyes open, one with eyes closed)\n",
    "- three two-minute runs of each of the four following tasks:\n",
    "    - 1:\n",
    "        - A target appears on either the left or the right side of the screen. \n",
    "        - The subject opens and closes the corresponding fist until the target disappears. \n",
    "        - Then the subject relaxes.\n",
    "    - 2:\n",
    "        - A target appears on either the left or the right side of the screen. \n",
    "        - The subject imagines opening and closing the corresponding fist until the target disappears. \n",
    "        - Then the subject relaxes.\n",
    "    - 3:\n",
    "        - A target appears on either the top or the bottom of the screen. \n",
    "        - The subject opens and closes either both fists (if the target is on top) or both feet (if the target is on the bottom) until the target disappears. \n",
    "        - Then the subject relaxes.\n",
    "    - 4:\n",
    "        - A target appears on either the top or the bottom of the screen. \n",
    "        - The subject imagines opening and closing either both fists (if the target is on top) or both feet (if the target is on the bottom) until the target disappears. \n",
    "        - Then the subject relaxes.\n",
    "\n",
    "The data are provided here in EDF+ format (containing 64 EEG signals, each sampled at 160 samples per second, and an annotation channel). \n",
    "For use with PhysioToolkit software, rdedfann generated a separate PhysioBank-compatible annotation file (with the suffix .event) for each recording. \n",
    "The .event files and the annotation channels in the corresponding .edf files contain identical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary tasks\n",
    "\n",
    "Remembering that:\n",
    "\n",
    "    - Task 1 (open and close left or right fist)\n",
    "    - Task 2 (imagine opening and closing left or right fist)\n",
    "    - Task 3 (open and close both fists or both feet)\n",
    "    - Task 4 (imagine opening and closing both fists or both feet)\n",
    "\n",
    "we will referred to 'Task *' with the meneaning above. \n",
    "\n",
    "In summary, the experimental runs were:\n",
    "\n",
    "1.  Baseline, eyes open\n",
    "2.  Baseline, eyes closed\n",
    "3.  Task 1 \n",
    "4.  Task -2 \n",
    "5.  Task --3 \n",
    "6.  Task ---4 \n",
    "7.  Task 1\n",
    "8.  Task -2\n",
    "9.  Task --3\n",
    "10. Task ---4\n",
    "11. Task 1\n",
    "12. Task -2\n",
    "13. Task --3\n",
    "14. Task ---4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation\n",
    "\n",
    "Each annotation includes one of three codes (T0, T1, or T2):\n",
    "\n",
    "- T0 corresponds to rest\n",
    "- T1 corresponds to onset of motion (real or imagined) of\n",
    "    - the left fist (in runs 3, 4, 7, 8, 11, and 12)\n",
    "    - both fists (in runs 5, 6, 9, 10, 13, and 14)\n",
    "- T2 corresponds to onset of motion (real or imagined) of\n",
    "    - the right fist (in runs 3, 4, 7, 8, 11, and 12)\n",
    "    - both feet (in runs 5, 6, 9, 10, 13, and 14)\n",
    "    \n",
    "In the BCI2000-format versions of these files, which may be available from the contributors of this data set, these annotations are encoded as values of 0, 1, or 2 in the TargetCode state variable.\n",
    "\n",
    "{'T0':0, 'T1':1, 'T2':2}\n",
    "\n",
    "In our experiments we will see only :\n",
    "\n",
    "- run_type_0:\n",
    "    - append_X\n",
    "- run_type_1\n",
    "    - append_X_y\n",
    "- run_type_2\n",
    "    - append_X_y\n",
    "    \n",
    "and the coding is: \n",
    "\n",
    "- T0 corresponds to rest \n",
    "    - (2)\n",
    "- T1 (real or imagined)\n",
    "    - (4,  8, 12) the left fist \n",
    "    - (6, 10, 14) both fists \n",
    "- T2 (real or imagined)\n",
    "    - (4,  8, 12) the right fist \n",
    "    - (6, 10, 14) both feet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Raw Data Import\n",
    "\n",
    "I will use a EEG data handling package named MNE (https://martinos.org/mne/stable/index.html) to import raw data and annotation for events from edf files. This package also provides essential signal analysis features, e.g. band-pass filtering. The raw data were filtered using 1Hz of high-pass filter.\n",
    "\n",
    "In this research, there are 5 classes for the data, imagined motion of:\n",
    "    - right fist, \n",
    "    - left fist, \n",
    "    - both fists, \n",
    "    - both feet,\n",
    "    - rest with eyes closed.\n",
    "\n",
    "A data (S089) from one of the 109 subjects was excluded as the record was severely corrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "The original goal of applying neural networks is to exclude hand-crafted algorithms & preprocessing as much as possible. I did not use any proprecessing techniques further than standardization to build an end-to-end classifer from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_train, y_train] = pickle.load( open( \"./py/stack/train.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_test, y_test] = pickle.load( open( \"./py/stack/test.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the EEG recording instrument has 3D locations over the subjects\\` scalp, it is essential for the model to learn from the spatial pattern as well as the temporal pattern. I transformed the data into 2D meshes that represents the locations of the electrodes so that stacked convolutional neural networks can grasp the spatial information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling - Time-Distributed CNN + RNN\n",
    "\n",
    "Training Plan:\n",
    "\n",
    "+ 4 GPU units (Nvidia Tesla P100) were used to train this neural network.\n",
    "+ Instead of training the whole model at once, I trained the first block (CNN) first. Then using the trained parameters as initial values, I trained the next blocks step-by-step. This approach can greatly reduce the time required for training and help avoiding falling into local minimums.\n",
    "+ The first blocks (CNN) can be applied for other EEG classification models as a pre-trained base.\n",
    "\n",
    "+ The initial learning rate is set to be $10^{3}$ with Adam optimization. I used several callbacks such as ReduceLROnPlateau which adjusts the learning rate at local minima. Also, I record the log for tensorboard to monitor the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.squeeze().reshape(*X_train.squeeze().shape, 1)\n",
    "X_test = X_test.squeeze().reshape(*X_test.squeeze().shape, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024056, 10, 10, 11, 1)\n",
      "(256014, 10, 10, 11, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make another dimension, 1, to apply CNN for each time frame.\n",
    "X_train = X_train.reshape(*X_train.shape, 1)\n",
    "X_test = X_test.reshape(*X_test.shape, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Keras Implementation\n",
    "\n",
    "The Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 10, 10, 11, 1)     0         \n",
      "_________________________________________________________________\n",
      "CNN1 (TimeDistributed)       (None, 10, 10, 11, 16)    160       \n",
      "_________________________________________________________________\n",
      "batch1 (BatchNormalization)  (None, 10, 10, 11, 16)    64        \n",
      "_________________________________________________________________\n",
      "act1 (Activation)            (None, 10, 10, 11, 16)    0         \n",
      "_________________________________________________________________\n",
      "CNN2 (TimeDistributed)       (None, 10, 10, 11, 32)    4640      \n",
      "_________________________________________________________________\n",
      "batch2 (BatchNormalization)  (None, 10, 10, 11, 32)    128       \n",
      "_________________________________________________________________\n",
      "act2 (Activation)            (None, 10, 10, 11, 32)    0         \n",
      "_________________________________________________________________\n",
      "CNN3 (TimeDistributed)       (None, 10, 10, 11, 64)    18496     \n",
      "_________________________________________________________________\n",
      "batch3 (BatchNormalization)  (None, 10, 10, 11, 64)    256       \n",
      "_________________________________________________________________\n",
      "act3 (Activation)            (None, 10, 10, 11, 64)    0         \n",
      "_________________________________________________________________\n",
      "flatten (TimeDistributed)    (None, 10, 7040)          0         \n",
      "_________________________________________________________________\n",
      "FC (Dense)                   (None, 10, 1024)          7209984   \n",
      "_________________________________________________________________\n",
      "dropout1 (Dropout)           (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "batch4 (BatchNormalization)  (None, 10, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "LSTM1 (LSTM)                 (None, 10, 64)            278784    \n",
      "_________________________________________________________________\n",
      "LSTM2 (LSTM)                 (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "FC2 (Dense)                  (None, 1024)              66560     \n",
      "_________________________________________________________________\n",
      "dropout2 (Dropout)           (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 3075      \n",
      "=================================================================\n",
      "Total params: 7,619,267\n",
      "Trainable params: 7,616,995\n",
      "Non-trainable params: 2,272\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Complicated Model - the same as Zhang`s\n",
    "input_shape = (10, 10, 11, 1)\n",
    "lecun = initializers.lecun_normal(seed=42)\n",
    "\n",
    "# TimeDistributed Wrapper\n",
    "def timeDist(layer, prev_layer, name):\n",
    "    return TimeDistributed(layer, name=name)(prev_layer)\n",
    "    \n",
    "# Input layer\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Convolutional layers block\n",
    "x = timeDist(Conv2D(16, (3,3), padding='same', data_format='channels_last', kernel_initializer=lecun), inputs, name='CNN1')\n",
    "x = BatchNormalization(name='batch1')(x)\n",
    "x = Activation('elu', name='act1')(x)\n",
    "x = timeDist(Conv2D(32, (3,3), padding='same', data_format='channels_last', kernel_initializer=lecun), x, name='CNN2')\n",
    "x = BatchNormalization(name='batch2')(x)\n",
    "x = Activation('elu', name='act2')(x)\n",
    "x = timeDist(Conv2D(64, (3,3), padding='same', data_format='channels_last', kernel_initializer=lecun), x, name='CNN3')\n",
    "x = BatchNormalization(name='batch3')(x)\n",
    "x = Activation('elu', name='act3')(x)\n",
    "x = timeDist(Flatten(), x, name='flatten')\n",
    "\n",
    "# Fully connected layer block\n",
    "y = Dense(1024, kernel_initializer=lecun, name='FC')(x)\n",
    "y = Dropout(0.5, name='dropout1')(y)\n",
    "y = BatchNormalization(name='batch4')(y)\n",
    "y = Activation(activation='elu')(y)\n",
    "\n",
    "# Recurrent layers block\n",
    "z = LSTM(64, kernel_initializer=lecun, return_sequences=True, name='LSTM1')(y)\n",
    "z = LSTM(64, kernel_initializer=lecun, name='LSTM2')(z)\n",
    "\n",
    "# Fully connected layer block\n",
    "h = Dense(1024, kernel_initializer=lecun, activation='elu', name='FC2')(z)\n",
    "h = Dropout(0.5, name='dropout2')(h)\n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(5, activation='softmax')(h)\n",
    "\n",
    "# Model compile\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./py/type1/model/model_base.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = load_model('./py/model/model_14_0.7098.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Load a model to transfer pre-trained parameters\n",
    "trans_model = model.load('CNN_3blocks.h5')\n",
    "\n",
    "# Transfer learning - parameter copy & paste\n",
    "which_layer = 'CNN1,CNN2,CNN3,batch1,batch2,batch3'.split(',')\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "trans_layer_names = [layer.name for layer in trans_model.layers]\n",
    "\n",
    "for layer in which_layer:\n",
    "    ind = layer_names.index(layer)\n",
    "    trans_ind = trans_layer_names.index(layer)\n",
    "    model.layers[ind].set_weights(trans_model.layers[trans_ind].get_weights())\n",
    "    \n",
    "for layer in model.layers[:9]: # Freeze the first 9 layers(CNN block)\n",
    "    layer.trainable = False\n",
    "    \n",
    "\n",
    "\n",
    "# Turn on multi-GPU mode\n",
    "model = multi_gpu_model(model, gpus=4)\n",
    "\n",
    "This metrics calculate sensitivity and specificity batch-wise.\n",
    "Keras development team removed this feature because\n",
    "these metrics should be understood as global metrics.\n",
    "\n",
    "\n",
    "I am not using it this time.\n",
    "\n",
    "# Metrics - sensitivity, specificity, accuracy\n",
    "def sens(y_true, y_pred): # Sensitivity\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def prec(y_true, y_pred): # Precision\n",
    "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValTensorBoard(TensorBoard):\n",
    "    '''\n",
    "    Plot training and validation losses on the same Tensorboard graph\n",
    "    Supersede Tensorboard callback\n",
    "    '''\n",
    "    def __init__(self, log_dir=\"./py/logs/\", **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [callbacks.ModelCheckpoint(\"./py/weights/weights_{epoch:02d}_{val_acc:.4f}.h5\", save_best_only=False, monitor='val_loss'),\n",
    "                 callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5),\n",
    "                 callbacks.CSVLogger(\"./py/logs/log.csv\", separator=',', append=True),\n",
    "                 TrainValTensorBoard()]\n",
    "\n",
    "# Start training\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.adam(lr=1e-4), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 819244 samples, validate on 204812 samples\n",
      "Epoch 1/500\n",
      "819244/819244 [==============================] - 1857s 2ms/step - loss: 0.8649 - acc: 0.5072 - val_loss: 0.8132 - val_acc: 0.5534\n",
      "Epoch 2/500\n",
      "819244/819244 [==============================] - 1845s 2ms/step - loss: 0.7711 - acc: 0.5772 - val_loss: 0.7690 - val_acc: 0.5929\n",
      "Epoch 3/500\n",
      "819244/819244 [==============================] - 1694s 2ms/step - loss: 0.7138 - acc: 0.6174 - val_loss: 0.7605 - val_acc: 0.6134\n",
      "Epoch 4/500\n",
      "819244/819244 [==============================] - 1371s 2ms/step - loss: 0.6739 - acc: 0.6431 - val_loss: 0.7312 - val_acc: 0.6324\n",
      "Epoch 5/500\n",
      "819244/819244 [==============================] - 1372s 2ms/step - loss: 0.6411 - acc: 0.6650 - val_loss: 0.7174 - val_acc: 0.6452\n",
      "Epoch 6/500\n",
      "819244/819244 [==============================] - 1372s 2ms/step - loss: 0.6138 - acc: 0.6822 - val_loss: 0.7320 - val_acc: 0.6507\n",
      "Epoch 7/500\n",
      "819244/819244 [==============================] - 1372s 2ms/step - loss: 0.5896 - acc: 0.6983 - val_loss: 0.7156 - val_acc: 0.6661\n",
      "Epoch 8/500\n",
      "819244/819244 [==============================] - 1793s 2ms/step - loss: 0.5672 - acc: 0.7124 - val_loss: 0.7323 - val_acc: 0.6673\n",
      "Epoch 9/500\n",
      "819244/819244 [==============================] - 1859s 2ms/step - loss: 0.5468 - acc: 0.7256 - val_loss: 0.7328 - val_acc: 0.6774\n",
      "Epoch 10/500\n",
      "139456/819244 [====>.........................] - ETA: 23:33 - loss: 0.5216 - acc: 0.7402"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=64, epochs=500, shuffle=True, \n",
    "                    validation_split=0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "history = model.fit(X_train[:1000], y_train[:1000], batch_size=1, epochs=1, shuffle=True, \n",
    "                    validation_split=0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in libraries\n",
    "import pickle\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directories\n",
    "if not os.path.exists('./py/metrics/'):\n",
    "    os.makedirs('./py/metrics/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history[loss_list[0]]) + 1)\n",
    "    \n",
    "   ## Loss\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history[l], 'b', label='Training loss (' + str(str(format(history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history[l], 'g', label='Validation loss (' + str(str(format(history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"./py/metrics/loss.png\")\n",
    "    \n",
    "    ## Accuracy\n",
    "    plt.figure(2)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history[l], 'b', label='Training accuracy (' + str(format(history[l][-1],'.5f'))+')')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history[l], 'g', label='Validation accuracy (' + str(format(history[l][-1],'.5f'))+')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(\"./py/metrics/acc.png\")\n",
    "    \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        title='Normalized confusion matrix'\n",
    "    else:\n",
    "        title='Confusion matrix'\n",
    "\n",
    "    plt.figure(3)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(\"./py/metrics/confuMat.png\")\n",
    "    plt.show()\n",
    "    \n",
    "def full_multiclass_report(model,\n",
    "                           x,\n",
    "                           y_true,\n",
    "                           classes):\n",
    "    \n",
    "    # 2. Predict classes and stores in y_pred\n",
    "    y_pred = model.predict(x).argmax(axis=1)\n",
    "    \n",
    "    # 3. Print accuracy score\n",
    "    print(\"Accuracy : \"+ str(accuracy_score(y_true,y_pred)))\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    # 4. Print classification report\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_true,y_pred,digits=4))    \n",
    "    \n",
    "    # 5. Plot confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_true,y_pred)\n",
    "    print(cnf_matrix)\n",
    "    plot_confusion_matrix(cnf_matrix,classes=classes)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "howManyTest = 0.2\n",
    "\n",
    "thisInd = np.random.randint(0, len(X_test), size=(len(X_test)//howManyTest))\n",
    "X_conf, y_conf = X_test[[i for i in thisInd], :], y_test[[i for i in thisInd],:] \n",
    "\n",
    "'''\n",
    "## Only if you have a previous model + history\n",
    "# Get the model\n",
    "model = models.load_model('./py/model/model_1230.h5')\n",
    "\n",
    "# Get the history\n",
    "with open('./history/history_1230.pkl', 'rb') as hist:\n",
    "    history = pickle.load(hist)\n",
    "'''\n",
    "\n",
    "# Get the graphics\n",
    "plot_history(history)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3], 1)\n",
    "full_multiclass_report(model,\n",
    "                       X_test,\n",
    "                       y_test.argmax(axis=1),\n",
    "                       [1,2,3,4,5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
