{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Classification - Dataset creation\n",
    "updated: Sep. 01, 2018\n",
    "\n",
    "Data: https://www.physionet.org/pn4/eegmmidb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Downloads\n",
    "\n",
    "### Warning: Executing these blocks will automatically create directories and download datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# System\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import urllib\n",
    "\n",
    "# Essential Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil, floor\n",
    "\n",
    "# Get Paths\n",
    "from glob import glob\n",
    "\n",
    "# EEG package\n",
    "from mne import pick_types, events_from_annotations\n",
    "from mne.io import read_raw_edf\n",
    "\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to take the dataset from physionet web site\n",
    "CONTEXT = 'pn4/'\n",
    "MATERIAL = 'eegmmidb/'\n",
    "URL = 'https://www.physionet.org/' + CONTEXT + MATERIAL\n",
    "\n",
    "# Change this directory according to your setting\n",
    "USERDIR = './dataset/raw_data/'\n",
    "\n",
    "page = requests.get(URL).text\n",
    "FOLDERS = sorted(list(set(re.findall(r'S[0-9]+', page))))\n",
    "\n",
    "URLS = [URL+x+'/' for x in FOLDERS]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if not os.path.exists(os.path.dirname(USERDIR)):\n",
    "    os.makedirs(os.path.dirname(USERDIR))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Warning: Executing this block will create folders\n",
    "for folder in FOLDERS:\n",
    "    pathlib.Path(USERDIR +'/'+ folder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Warning: Executing this block will start downloading data\n",
    "for i, folder in enumerate(FOLDERS):\n",
    "    page = requests.get(URLS[i]).text\n",
    "    subs = list(set(re.findall(r'S[0-9]+R[0-9]+', page)))\n",
    "    \n",
    "    print('Working on {}, {:.1%} completed'.format(folder, (i+1)/len(FOLDERS)))\n",
    "    for sub in subs:\n",
    "        urllib.request.urlretrieve(URLS[i]+sub+'.edf', os.path.join(USERDIR, folder, sub+'.edf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "Subjects performed different motor/imagery tasks while 64-channel EEG were recorded using the BCI2000 system (http://www.bci2000.org). Each subject performed 14 experimental runs: \n",
    "\n",
    "- two one-minute baseline runs (one with eyes open, one with eyes closed)\n",
    "- three two-minute runs of each of the four following tasks:\n",
    "    - 1:\n",
    "        - A target appears on either the left or the right side of the screen. \n",
    "        - The subject opens and closes the corresponding fist until the target disappears. \n",
    "        - Then the subject relaxes.\n",
    "    - 2:\n",
    "        - A target appears on either the left or the right side of the screen. \n",
    "        - The subject imagines opening and closing the corresponding fist until the target disappears. \n",
    "        - Then the subject relaxes.\n",
    "    - 3:\n",
    "        - A target appears on either the top or the bottom of the screen. \n",
    "        - The subject opens and closes either both fists (if the target is on top) or both feet (if the target is on the bottom) until the target disappears. \n",
    "        - Then the subject relaxes.\n",
    "    - 4:\n",
    "        - A target appears on either the top or the bottom of the screen. \n",
    "        - The subject imagines opening and closing either both fists (if the target is on top) or both feet (if the target is on the bottom) until the target disappears. \n",
    "        - Then the subject relaxes.\n",
    "\n",
    "The data are provided here in EDF+ format (containing 64 EEG signals, each sampled at 160 samples per second, and an annotation channel). \n",
    "For use with PhysioToolkit software, rdedfann generated a separate PhysioBank-compatible annotation file (with the suffix .event) for each recording. \n",
    "The .event files and the annotation channels in the corresponding .edf files contain identical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary tasks\n",
    "\n",
    "Remembering that:\n",
    "\n",
    "    - Task 1 (open and close left or right fist)\n",
    "    - Task 2 (imagine opening and closing left or right fist)\n",
    "    - Task 3 (open and close both fists or both feet)\n",
    "    - Task 4 (imagine opening and closing both fists or both feet)\n",
    "\n",
    "we will referred to 'Task *' with the meneaning above. \n",
    "\n",
    "In summary, the experimental runs were:\n",
    "\n",
    "1.  Baseline, eyes open\n",
    "2.  Baseline, eyes closed\n",
    "3.  Task 1 \n",
    "4.  Task -2 \n",
    "5.  Task --3 \n",
    "6.  Task ---4 \n",
    "7.  Task 1\n",
    "8.  Task -2\n",
    "9.  Task --3\n",
    "10. Task ---4\n",
    "11. Task 1\n",
    "12. Task -2\n",
    "13. Task --3\n",
    "14. Task ---4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation\n",
    "\n",
    "Each annotation includes one of three codes (T0, T1, or T2):\n",
    "\n",
    "- T0 corresponds to rest\n",
    "- T1 corresponds to onset of motion (real or imagined) of\n",
    "    - the left fist (in runs 3, 4, 7, 8, 11, and 12)\n",
    "    - both fists (in runs 5, 6, 9, 10, 13, and 14)\n",
    "- T2 corresponds to onset of motion (real or imagined) of\n",
    "    - the right fist (in runs 3, 4, 7, 8, 11, and 12)\n",
    "    - both feet (in runs 5, 6, 9, 10, 13, and 14)\n",
    "    \n",
    "In the BCI2000-format versions of these files, which may be available from the contributors of this data set, these annotations are encoded as values of 0, 1, or 2 in the TargetCode state variable.\n",
    "\n",
    "{'T0':0, 'T1':1, 'T2':2}\n",
    "\n",
    "In our experiments we will see only :\n",
    "\n",
    "- run_type_0:\n",
    "    - append_X\n",
    "- run_type_1\n",
    "    - append_X_y\n",
    "- run_type_2\n",
    "    - append_X_y\n",
    "    \n",
    "and the coding is: \n",
    "\n",
    "- T0 corresponds to rest \n",
    "    - (2)\n",
    "- T1 (real or imagined)\n",
    "    - (4,  8, 12) the left fist \n",
    "    - (6, 10, 14) both fists \n",
    "- T2 (real or imagined)\n",
    "    - (4,  8, 12) the right fist \n",
    "    - (6, 10, 14) both feet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Raw Data Import\n",
    "\n",
    "I will use a EEG data handling package named MNE (https://martinos.org/mne/stable/index.html) to import raw data and annotation for events from edf files. This package also provides essential signal analysis features, e.g. band-pass filtering. The raw data were filtered using 1Hz of high-pass filter.\n",
    "\n",
    "In this research, there are 5 classes for the data, imagined motion of:\n",
    "    - right fist, \n",
    "    - left fist, \n",
    "    - both fists, \n",
    "    - both feet,\n",
    "    - rest with eyes closed.\n",
    "\n",
    "A data (S089) from one of the 109 subjects was excluded as the record was severely corrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file paths\n",
    "PATH = './dataset/raw_data/'\n",
    "SUBS = glob(PATH + 'S[0-9]*')\n",
    "FNAMES = sorted([x[-4:] for x in SUBS])\n",
    "\n",
    "REMOVE = ['S088', 'S089', 'S092', 'S100']\n",
    "\n",
    "# Remove subject 'S089' with damaged data and 'S088', 'S092', 'S100' with 128Hz sampling rate (we want 160Hz)\n",
    "FNAMES = [ x for x in FNAMES if x not in REMOVE] \n",
    "\n",
    "emb = {'T0': 1, 'T1': 2, 'T2': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'Imaged'\n",
    "# data_type = 'Real'\n",
    "user_independent = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_get_data(data_type, subj_num=FNAMES, epoch_sec=0.0625):\n",
    "    \"\"\" Import from edf files data and targets in the shape of 3D tensor\n",
    "    \n",
    "        Output shape: (Trial*Channel*TimeFrames)\n",
    "        \n",
    "        Some edf+ files recorded at low sampling rate, 128Hz, are excluded. \n",
    "        Majority was sampled at 160Hz.\n",
    "        \n",
    "        epoch_sec: time interval for one segment of mashes (0.0625 is 1/16 as a fraction)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Event codes mean different actions for two groups of runs\n",
    "    if data_type == 'Real':\n",
    "        run_type_0 = '01'.split(',')\n",
    "        run_type_1 = '03,07,11'.split(',')\n",
    "        run_type_2 = '05,09,13'.split(',')\n",
    "    else:\n",
    "        run_type_0 = '02'.split(',')\n",
    "        run_type_1 = '04,08,12'.split(',')\n",
    "        run_type_2 = '06,10,14'.split(',')\n",
    "    \n",
    "    # Initiate X, y\n",
    "    X = []\n",
    "    y = []\n",
    "    p = []\n",
    "    dim = dict()\n",
    "    \n",
    "    # To compute the completion rate\n",
    "    count = len(subj_num)\n",
    "    \n",
    "    # fixed numbers\n",
    "    nChan = 64 \n",
    "    sfreq = 160\n",
    "    sliding = epoch_sec/2 \n",
    "    timeFromQue = 0.5\n",
    "    timeExercise = 4.1 #secomds\n",
    "    magic_number = 51\n",
    "    \n",
    "    run_0_segments = int(magic_number * (magic_number*timeFromQue))\n",
    "    run_segments = magic_number\n",
    "\n",
    "    # Sub-function to assign X and X, y\n",
    "    def append_X(n_segments, data, event=[]):\n",
    "        # Data should be changed\n",
    "        '''This function generate a tensor for X and append it to the existing X'''\n",
    "    \n",
    "        def window(n):\n",
    "            # (80) + (160 * 1/16 * n) \n",
    "            windowStart = int(timeFromQue*sfreq) + int(sfreq*sliding*n) \n",
    "            # (80) + (160 * 1/16 * (n+2))\n",
    "            windowEnd = int(timeFromQue*sfreq) + int(sfreq*sliding*(n+2)) \n",
    "            \n",
    "            while (windowEnd - windowStart) != sfreq*epoch_sec:\n",
    "                windowEnd += int(sfreq*epoch_sec) - (windowEnd - windowStart)\n",
    "                \n",
    "            return [windowStart, windowEnd]\n",
    "        \n",
    "        new_x = []\n",
    "        for n in range(n_segments):\n",
    "            # print('data[:, ',window(n)[0],':',window(n)[1],'].shape = ', data[:, window(n)[0]:window(n)[1]].shape, '(',nChan,',',int(sfreq*epoch_sec),')')\n",
    "            \n",
    "            if data[:, window(n)[0]:window(n)[1]].shape==(nChan, int(sfreq*epoch_sec)):\n",
    "                new_x.append(data[:, window(n)[0]: window(n)[1]])\n",
    "                 \n",
    "        return new_x\n",
    "    \n",
    "    def append_X_Y(p, run_type, event, old_x, old_y, old_p, data):\n",
    "        '''This function seperate the type of events \n",
    "        (refer to the data descriptitons for the list of the types)\n",
    "        Then assign X and Y according to the event types'''\n",
    "        # Number of sliding windows\n",
    "\n",
    "        # print('data', data.shape[1])\n",
    "        n_segments = run_segments\n",
    "        #n_segments = floor(data.shape[1]/(epoch_sec*sfreq*timeFromQue) - 1/epoch_sec - 1)\n",
    "        # print('run_'+str(run_type),' n_segments', n_segments, 'data', data.shape)\n",
    "        \n",
    "        # Rest excluded\n",
    "        if event[2] == emb['T0']:\n",
    "            return old_x, old_y, old_p\n",
    "        \n",
    "        # y assignment\n",
    "        if run_type == 1:\n",
    "            temp_y = [1] if event[2] == emb['T1'] else [2]\n",
    "        \n",
    "        elif run_type == 2:\n",
    "            temp_y = [3] if event[2] == emb['T1'] else [4]\n",
    "            \n",
    "        # print('event[2]', event[2], 'run_type', run_type, 'temp_y', temp_y)            \n",
    "        \n",
    "        # print('timeExercise * sfreq', timeExercise*sfreq, ' ?= 656')\n",
    "        new_x = append_X(n_segments, data, event)\n",
    "        new_y = old_y + temp_y*len(new_x)\n",
    "        new_p = old_p + p*len(new_x)\n",
    "        \n",
    "        return old_x + new_x, new_y, new_p\n",
    "    \n",
    "    # Iterate over subj_num: S001, S002, S003, ...\n",
    "    for i, subj in enumerate(subj_num):\n",
    "        # print('subj', subj)\n",
    "\n",
    "        \n",
    "        # Return completion rate\n",
    "        if i%((len(subj_num)//10)+1) == 0:\n",
    "            print('\\n')\n",
    "            print('working on {}, {:.0%} completed'.format(subj, i/count))\n",
    "            print('\\n')\n",
    "        \n",
    "        old_size = np.array(y).shape[0]\n",
    "        # print('subj:', subj, '| y.shape', np.array(y).shape ,'| X.shape', np.array(X).shape)\n",
    "\n",
    "        # Get file names\n",
    "        fnames = glob(os.path.join(PATH, subj, subj+'R*.edf'))\n",
    "        # Hold only the files that have an even number\n",
    "        fnames = sorted([name for name in fnames if name[-6:-4] in run_type_0+run_type_1+run_type_2])\n",
    "\n",
    "        # for each of ['02', '04', '06', '08', '12', '14']\n",
    "        for i, fname in enumerate(fnames):\n",
    "            # print('fname', fname)\n",
    "            \n",
    "            # Import data into MNE raw object\n",
    "            raw = read_raw_edf(fname, preload=True, verbose=False)\n",
    "            \n",
    "            picks = pick_types(raw.info, eeg=True)\n",
    "            # print('n_times', raw.n_times)\n",
    "            \n",
    "            if raw.info['sfreq'] != 160:\n",
    "                print('{} is sampled at 128Hz so will be excluded.'.format(subj))\n",
    "                break\n",
    "            \n",
    "            \n",
    "            # High-pass filtering\n",
    "            raw.filter(l_freq=1, h_freq=None, picks=picks)\n",
    "\n",
    "            # Get annotation\n",
    "            try:\n",
    "                events = events_from_annotations(raw, verbose=False)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # Get data\n",
    "            data = raw.get_data(picks=picks)\n",
    "\n",
    "            # print('event.shape', np.array(events[0]).shape, '| data.shape', data.shape)\n",
    "\n",
    "            # Number of this run\n",
    "            which_run = fname[-6:-4]\n",
    "\n",
    "            \"\"\" Assignment Starts \"\"\" \n",
    "            # run 1 - baseline (eye closed)\n",
    "            if which_run in run_type_0:\n",
    "\n",
    "                # Number of sliding windows\n",
    "                n_segments = run_0_segments\n",
    "                # n_segments = floor(data.shape[1]/(epoch_sec*sfreq*timeFromQue) - 1/epoch_sec - 1)\n",
    "                # print('run_0 n_segments', n_segments, 'data', data.shape)\n",
    "\n",
    "                # Append 0`s based on number of windows\n",
    "                new_X = append_X(n_segments, data)\n",
    "                X += new_X\n",
    "                y.extend([0] * len(new_X))\n",
    "                p.extend([subj]* len(new_X))\n",
    "                # print(events[0])   \n",
    "\n",
    "            # run 4,8,12 - imagine opening and closing left or right fist    \n",
    "            elif which_run in run_type_1:\n",
    "\n",
    "                for i, event in enumerate(events[0]):\n",
    "\n",
    "                    X, y, p = append_X_Y([subj], run_type=1, event=event, old_x=X, old_y=y, old_p=p, data=data[:, int(event[0]) : int(event[0] + timeExercise*sfreq)])\n",
    "                    # print(event)   \n",
    "\n",
    "            # run 6,10,14 - imagine opening and closing both fists or both feet\n",
    "            elif which_run in run_type_2:\n",
    "\n",
    "                for i, event in enumerate(events[0]):      \n",
    "\n",
    "                    X, y, p = append_X_Y([subj], run_type=2, event=event, old_x=X, old_y=y, old_p=p, data=data[:, int(event[0]) : int(event[0] + timeExercise*sfreq)])\n",
    "                    # print(event)    \n",
    "\n",
    "        print('subj:', subj, '|', np.array(y).shape[0] - old_size, '| y.shape', np.array(y).shape ,'| X.shape', np.array(X).shape, '| p.shape', np.array(p).shape)\n",
    "        dim[subj] =  np.array(y).shape[0] - old_size\n",
    "    print(np.array(X).shape)\n",
    "\n",
    "    X = np.stack(X)\n",
    "    y = np.array(y).reshape((-1,1))\n",
    "    p = np.array(p).reshape((-1,1))\n",
    "    return X, y, p, dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "working on S001, 0% completed\n",
      "\n",
      "\n",
      "subj: S001 | 5890 | y.shape (5890,) | X.shape (5890, 64, 10) | p.shape (5890,)\n",
      "subj: S002 | 5890 | y.shape (11780,) | X.shape (11780, 64, 10) | p.shape (11780,)\n",
      "subj: S003 | 5890 | y.shape (17670,) | X.shape (17670, 64, 10) | p.shape (17670,)\n",
      "subj: S004 | 5890 | y.shape (23560,) | X.shape (23560, 64, 10) | p.shape (23560,)\n",
      "subj: S005 | 5890 | y.shape (29450,) | X.shape (29450, 64, 10) | p.shape (29450,)\n",
      "subj: S006 | 5890 | y.shape (35340,) | X.shape (35340, 64, 10) | p.shape (35340,)\n",
      "subj: S007 | 5890 | y.shape (41230,) | X.shape (41230, 64, 10) | p.shape (41230,)\n",
      "subj: S008 | 5890 | y.shape (47120,) | X.shape (47120, 64, 10) | p.shape (47120,)\n",
      "subj: S009 | 5890 | y.shape (53010,) | X.shape (53010, 64, 10) | p.shape (53010,)\n",
      "subj: S010 | 5890 | y.shape (58900,) | X.shape (58900, 64, 10) | p.shape (58900,)\n",
      "subj: S011 | 5890 | y.shape (64790,) | X.shape (64790, 64, 10) | p.shape (64790,)\n",
      "\n",
      "\n",
      "working on S012, 10% completed\n",
      "\n",
      "\n",
      "subj: S012 | 5890 | y.shape (70680,) | X.shape (70680, 64, 10) | p.shape (70680,)\n",
      "subj: S013 | 5890 | y.shape (76570,) | X.shape (76570, 64, 10) | p.shape (76570,)\n",
      "subj: S014 | 5890 | y.shape (82460,) | X.shape (82460, 64, 10) | p.shape (82460,)\n",
      "subj: S015 | 5890 | y.shape (88350,) | X.shape (88350, 64, 10) | p.shape (88350,)\n",
      "subj: S016 | 5890 | y.shape (94240,) | X.shape (94240, 64, 10) | p.shape (94240,)\n",
      "subj: S017 | 5890 | y.shape (100130,) | X.shape (100130, 64, 10) | p.shape (100130,)\n",
      "subj: S018 | 5890 | y.shape (106020,) | X.shape (106020, 64, 10) | p.shape (106020,)\n",
      "subj: S019 | 5890 | y.shape (111910,) | X.shape (111910, 64, 10) | p.shape (111910,)\n",
      "subj: S020 | 5890 | y.shape (117800,) | X.shape (117800, 64, 10) | p.shape (117800,)\n",
      "subj: S021 | 5890 | y.shape (123690,) | X.shape (123690, 64, 10) | p.shape (123690,)\n",
      "subj: S022 | 5890 | y.shape (129580,) | X.shape (129580, 64, 10) | p.shape (129580,)\n",
      "\n",
      "\n",
      "working on S023, 21% completed\n",
      "\n",
      "\n",
      "subj: S023 | 5890 | y.shape (135470,) | X.shape (135470, 64, 10) | p.shape (135470,)\n",
      "subj: S024 | 5890 | y.shape (141360,) | X.shape (141360, 64, 10) | p.shape (141360,)\n",
      "subj: S025 | 5890 | y.shape (147250,) | X.shape (147250, 64, 10) | p.shape (147250,)\n",
      "subj: S026 | 5890 | y.shape (153140,) | X.shape (153140, 64, 10) | p.shape (153140,)\n",
      "subj: S027 | 5890 | y.shape (159030,) | X.shape (159030, 64, 10) | p.shape (159030,)\n",
      "subj: S028 | 5890 | y.shape (164920,) | X.shape (164920, 64, 10) | p.shape (164920,)\n",
      "subj: S029 | 5890 | y.shape (170810,) | X.shape (170810, 64, 10) | p.shape (170810,)\n",
      "subj: S030 | 5890 | y.shape (176700,) | X.shape (176700, 64, 10) | p.shape (176700,)\n",
      "subj: S031 | 5890 | y.shape (182590,) | X.shape (182590, 64, 10) | p.shape (182590,)\n",
      "subj: S032 | 5890 | y.shape (188480,) | X.shape (188480, 64, 10) | p.shape (188480,)\n",
      "subj: S033 | 5890 | y.shape (194370,) | X.shape (194370, 64, 10) | p.shape (194370,)\n",
      "\n",
      "\n",
      "working on S034, 31% completed\n",
      "\n",
      "\n",
      "subj: S034 | 5890 | y.shape (200260,) | X.shape (200260, 64, 10) | p.shape (200260,)\n",
      "subj: S035 | 5890 | y.shape (206150,) | X.shape (206150, 64, 10) | p.shape (206150,)\n",
      "subj: S036 | 5890 | y.shape (212040,) | X.shape (212040, 64, 10) | p.shape (212040,)\n",
      "subj: S037 | 5890 | y.shape (217930,) | X.shape (217930, 64, 10) | p.shape (217930,)\n",
      "subj: S038 | 5890 | y.shape (223820,) | X.shape (223820, 64, 10) | p.shape (223820,)\n",
      "subj: S039 | 5890 | y.shape (229710,) | X.shape (229710, 64, 10) | p.shape (229710,)\n",
      "subj: S040 | 5890 | y.shape (235600,) | X.shape (235600, 64, 10) | p.shape (235600,)\n",
      "subj: S041 | 5890 | y.shape (241490,) | X.shape (241490, 64, 10) | p.shape (241490,)\n",
      "subj: S042 | 5890 | y.shape (247380,) | X.shape (247380, 64, 10) | p.shape (247380,)\n",
      "subj: S043 | 5890 | y.shape (253270,) | X.shape (253270, 64, 10) | p.shape (253270,)\n",
      "subj: S044 | 5890 | y.shape (259160,) | X.shape (259160, 64, 10) | p.shape (259160,)\n",
      "\n",
      "\n",
      "working on S045, 42% completed\n",
      "\n",
      "\n",
      "subj: S045 | 5890 | y.shape (265050,) | X.shape (265050, 64, 10) | p.shape (265050,)\n",
      "subj: S046 | 5890 | y.shape (270940,) | X.shape (270940, 64, 10) | p.shape (270940,)\n",
      "subj: S047 | 5890 | y.shape (276830,) | X.shape (276830, 64, 10) | p.shape (276830,)\n",
      "subj: S048 | 5890 | y.shape (282720,) | X.shape (282720, 64, 10) | p.shape (282720,)\n",
      "subj: S049 | 5890 | y.shape (288610,) | X.shape (288610, 64, 10) | p.shape (288610,)\n",
      "subj: S050 | 5890 | y.shape (294500,) | X.shape (294500, 64, 10) | p.shape (294500,)\n",
      "subj: S051 | 5890 | y.shape (300390,) | X.shape (300390, 64, 10) | p.shape (300390,)\n",
      "subj: S052 | 5890 | y.shape (306280,) | X.shape (306280, 64, 10) | p.shape (306280,)\n",
      "subj: S053 | 5890 | y.shape (312170,) | X.shape (312170, 64, 10) | p.shape (312170,)\n",
      "subj: S054 | 5890 | y.shape (318060,) | X.shape (318060, 64, 10) | p.shape (318060,)\n",
      "subj: S055 | 5890 | y.shape (323950,) | X.shape (323950, 64, 10) | p.shape (323950,)\n",
      "\n",
      "\n",
      "working on S056, 52% completed\n",
      "\n",
      "\n",
      "subj: S056 | 5890 | y.shape (329840,) | X.shape (329840, 64, 10) | p.shape (329840,)\n",
      "subj: S057 | 5890 | y.shape (335730,) | X.shape (335730, 64, 10) | p.shape (335730,)\n",
      "subj: S058 | 5890 | y.shape (341620,) | X.shape (341620, 64, 10) | p.shape (341620,)\n",
      "subj: S059 | 5890 | y.shape (347510,) | X.shape (347510, 64, 10) | p.shape (347510,)\n",
      "subj: S060 | 5890 | y.shape (353400,) | X.shape (353400, 64, 10) | p.shape (353400,)\n",
      "subj: S061 | 5890 | y.shape (359290,) | X.shape (359290, 64, 10) | p.shape (359290,)\n",
      "subj: S062 | 5890 | y.shape (365180,) | X.shape (365180, 64, 10) | p.shape (365180,)\n",
      "subj: S063 | 5890 | y.shape (371070,) | X.shape (371070, 64, 10) | p.shape (371070,)\n",
      "subj: S064 | 5890 | y.shape (376960,) | X.shape (376960, 64, 10) | p.shape (376960,)\n",
      "subj: S065 | 5890 | y.shape (382850,) | X.shape (382850, 64, 10) | p.shape (382850,)\n",
      "subj: S066 | 5890 | y.shape (388740,) | X.shape (388740, 64, 10) | p.shape (388740,)\n",
      "\n",
      "\n",
      "working on S067, 63% completed\n",
      "\n",
      "\n",
      "subj: S067 | 5890 | y.shape (394630,) | X.shape (394630, 64, 10) | p.shape (394630,)\n",
      "subj: S068 | 5890 | y.shape (400520,) | X.shape (400520, 64, 10) | p.shape (400520,)\n",
      "subj: S069 | 5890 | y.shape (406410,) | X.shape (406410, 64, 10) | p.shape (406410,)\n",
      "subj: S070 | 5890 | y.shape (412300,) | X.shape (412300, 64, 10) | p.shape (412300,)\n",
      "subj: S071 | 5890 | y.shape (418190,) | X.shape (418190, 64, 10) | p.shape (418190,)\n",
      "subj: S072 | 5890 | y.shape (424080,) | X.shape (424080, 64, 10) | p.shape (424080,)\n",
      "subj: S073 | 5890 | y.shape (429970,) | X.shape (429970, 64, 10) | p.shape (429970,)\n",
      "subj: S074 | 5890 | y.shape (435860,) | X.shape (435860, 64, 10) | p.shape (435860,)\n",
      "subj: S075 | 5890 | y.shape (441750,) | X.shape (441750, 64, 10) | p.shape (441750,)\n",
      "subj: S076 | 5890 | y.shape (447640,) | X.shape (447640, 64, 10) | p.shape (447640,)\n",
      "subj: S077 | 5890 | y.shape (453530,) | X.shape (453530, 64, 10) | p.shape (453530,)\n",
      "\n",
      "\n",
      "working on S078, 73% completed\n",
      "\n",
      "\n",
      "subj: S078 | 5890 | y.shape (459420,) | X.shape (459420, 64, 10) | p.shape (459420,)\n",
      "subj: S079 | 5890 | y.shape (465310,) | X.shape (465310, 64, 10) | p.shape (465310,)\n",
      "subj: S080 | 5890 | y.shape (471200,) | X.shape (471200, 64, 10) | p.shape (471200,)\n",
      "subj: S081 | 5890 | y.shape (477090,) | X.shape (477090, 64, 10) | p.shape (477090,)\n",
      "subj: S082 | 5890 | y.shape (482980,) | X.shape (482980, 64, 10) | p.shape (482980,)\n",
      "subj: S083 | 5890 | y.shape (488870,) | X.shape (488870, 64, 10) | p.shape (488870,)\n",
      "subj: S084 | 5890 | y.shape (494760,) | X.shape (494760, 64, 10) | p.shape (494760,)\n",
      "subj: S085 | 5890 | y.shape (500650,) | X.shape (500650, 64, 10) | p.shape (500650,)\n",
      "subj: S086 | 5890 | y.shape (506540,) | X.shape (506540, 64, 10) | p.shape (506540,)\n",
      "subj: S087 | 5890 | y.shape (512430,) | X.shape (512430, 64, 10) | p.shape (512430,)\n",
      "subj: S090 | 5890 | y.shape (518320,) | X.shape (518320, 64, 10) | p.shape (518320,)\n",
      "\n",
      "\n",
      "working on S091, 84% completed\n",
      "\n",
      "\n",
      "subj: S091 | 5890 | y.shape (524210,) | X.shape (524210, 64, 10) | p.shape (524210,)\n",
      "subj: S093 | 5890 | y.shape (530100,) | X.shape (530100, 64, 10) | p.shape (530100,)\n",
      "subj: S094 | 5890 | y.shape (535990,) | X.shape (535990, 64, 10) | p.shape (535990,)\n",
      "subj: S095 | 5890 | y.shape (541880,) | X.shape (541880, 64, 10) | p.shape (541880,)\n",
      "subj: S096 | 5890 | y.shape (547770,) | X.shape (547770, 64, 10) | p.shape (547770,)\n",
      "subj: S097 | 5890 | y.shape (553660,) | X.shape (553660, 64, 10) | p.shape (553660,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj: S098 | 5890 | y.shape (559550,) | X.shape (559550, 64, 10) | p.shape (559550,)\n",
      "subj: S099 | 5890 | y.shape (565440,) | X.shape (565440, 64, 10) | p.shape (565440,)\n",
      "subj: S101 | 5890 | y.shape (571330,) | X.shape (571330, 64, 10) | p.shape (571330,)\n",
      "subj: S102 | 5890 | y.shape (577220,) | X.shape (577220, 64, 10) | p.shape (577220,)\n",
      "subj: S103 | 5890 | y.shape (583110,) | X.shape (583110, 64, 10) | p.shape (583110,)\n",
      "\n",
      "\n",
      "working on S104, 94% completed\n",
      "\n",
      "\n",
      "subj: S104 | 5788 | y.shape (588898,) | X.shape (588898, 64, 10) | p.shape (588898,)\n",
      "subj: S105 | 5890 | y.shape (594788,) | X.shape (594788, 64, 10) | p.shape (594788,)\n",
      "subj: S106 | 5890 | y.shape (600678,) | X.shape (600678, 64, 10) | p.shape (600678,)\n",
      "subj: S107 | 5890 | y.shape (606568,) | X.shape (606568, 64, 10) | p.shape (606568,)\n",
      "subj: S108 | 5890 | y.shape (612458,) | X.shape (612458, 64, 10) | p.shape (612458,)\n",
      "subj: S109 | 5890 | y.shape (618348,) | X.shape (618348, 64, 10) | p.shape (618348,)\n",
      "(618348, 64, 10)\n"
     ]
    }
   ],
   "source": [
    "X, y, p, dim = my_get_data(data_type, FNAMES, epoch_sec=0.0625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(618348, 64, 10)\n",
      "(618348, 1)\n",
      "(618348, 1)\n",
      "{'S001': 5890, 'S002': 5890, 'S003': 5890, 'S004': 5890, 'S005': 5890, 'S006': 5890, 'S007': 5890, 'S008': 5890, 'S009': 5890, 'S010': 5890, 'S011': 5890, 'S012': 5890, 'S013': 5890, 'S014': 5890, 'S015': 5890, 'S016': 5890, 'S017': 5890, 'S018': 5890, 'S019': 5890, 'S020': 5890, 'S021': 5890, 'S022': 5890, 'S023': 5890, 'S024': 5890, 'S025': 5890, 'S026': 5890, 'S027': 5890, 'S028': 5890, 'S029': 5890, 'S030': 5890, 'S031': 5890, 'S032': 5890, 'S033': 5890, 'S034': 5890, 'S035': 5890, 'S036': 5890, 'S037': 5890, 'S038': 5890, 'S039': 5890, 'S040': 5890, 'S041': 5890, 'S042': 5890, 'S043': 5890, 'S044': 5890, 'S045': 5890, 'S046': 5890, 'S047': 5890, 'S048': 5890, 'S049': 5890, 'S050': 5890, 'S051': 5890, 'S052': 5890, 'S053': 5890, 'S054': 5890, 'S055': 5890, 'S056': 5890, 'S057': 5890, 'S058': 5890, 'S059': 5890, 'S060': 5890, 'S061': 5890, 'S062': 5890, 'S063': 5890, 'S064': 5890, 'S065': 5890, 'S066': 5890, 'S067': 5890, 'S068': 5890, 'S069': 5890, 'S070': 5890, 'S071': 5890, 'S072': 5890, 'S073': 5890, 'S074': 5890, 'S075': 5890, 'S076': 5890, 'S077': 5890, 'S078': 5890, 'S079': 5890, 'S080': 5890, 'S081': 5890, 'S082': 5890, 'S083': 5890, 'S084': 5890, 'S085': 5890, 'S086': 5890, 'S087': 5890, 'S090': 5890, 'S091': 5890, 'S093': 5890, 'S094': 5890, 'S095': 5890, 'S096': 5890, 'S097': 5890, 'S098': 5890, 'S099': 5890, 'S101': 5890, 'S102': 5890, 'S103': 5890, 'S104': 5788, 'S105': 5890, 'S106': 5890, 'S107': 5890, 'S108': 5890, 'S109': 5890}\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(p.shape)\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( X , open( \"./dataset/processed_data/\"+data_type+\"/X.p\", \"wb\" ) , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( y , open( \"./dataset/processed_data/\"+data_type+\"/y.p\", \"wb\" ) , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( p , open( \"./dataset/processed_data/\"+data_type+\"/p.p\", \"wb\" ) , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( dim , open( \"./dataset/processed_data/\"+data_type+\"/dim.p\", \"wb\" ) , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pickle.load( open( \"./dataset/processed_data/\"+data_type+\"/X.p\", \"rb\" ) )\n",
    "y = pickle.load( open( \"./dataset/processed_data/\"+data_type+\"/y.p\", \"rb\" ) )\n",
    "p = pickle.load( open( \"./dataset/processed_data/\"+data_type+\"/p.p\", \"rb\" ) )\n",
    "dim = pickle.load( open( \"./dataset/processed_data/\"+data_type+\"/dim.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X', X.shape,'y', y.shape,'p',p.shape,'dim',len(dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "The original goal of applying neural networks is to exclude hand-crafted algorithms & preprocessing as much as possible. I did not use any proprecessing techniques further than standardization to build an end-to-end classifer from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, scale\n",
    "from collections import defaultdict\n",
    "\n",
    "def convert_mesh(X):\n",
    "    \n",
    "    mesh = np.zeros((X.shape[0], X.shape[2], 10, 11, 1))\n",
    "    X = np.swapaxes(X, 1, 2)\n",
    "    \n",
    "    # 1st line\n",
    "    mesh[:, :, 0, 4:7, 0] = X[:,:,21:24]; print('1st finished')\n",
    "    \n",
    "    # 2nd line\n",
    "    mesh[:, :, 1, 3:8, 0] = X[:,:,24:29]; print('2nd finished')\n",
    "    \n",
    "    # 3rd line\n",
    "    mesh[:, :, 2, 1:10, 0] = X[:,:,29:38]; print('3rd finished')\n",
    "    \n",
    "    # 4th line\n",
    "    mesh[:, :, 3, 1:10, 0] = np.concatenate((X[:,:,38].reshape(-1, X.shape[1], 1),\\\n",
    "                                          X[:,:,0:7], X[:,:,39].reshape(-1, X.shape[1], 1)), axis=2)\n",
    "    print('4th finished')\n",
    "    \n",
    "    # 5th line\n",
    "    mesh[:, :, 4, 0:11, 0] = np.concatenate((X[:,:,(42, 40)],\\\n",
    "                                        X[:,:,7:14], X[:,:,(41, 43)]), axis=2)\n",
    "    print('5th finished')\n",
    "    \n",
    "    # 6th line\n",
    "    mesh[:, :, 5, 1:10, 0] = np.concatenate((X[:,:,44].reshape(-1, X.shape[1], 1),\\\n",
    "                                        X[:,:,14:21], X[:,:,45].reshape(-1, X.shape[1], 1)), axis=2)\n",
    "    print('6th finished')\n",
    "               \n",
    "    # 7th line\n",
    "    mesh[:, :, 6, 1:10, 0] = X[:,:,46:55]; print('7th finished')\n",
    "    \n",
    "    # 8th line\n",
    "    mesh[:, :, 7, 3:8, 0] = X[:,:,55:60]; print('8th finished')\n",
    "    \n",
    "    # 9th line\n",
    "    mesh[:, :, 8, 4:7, 0] = X[:,:,60:63]; print('9th finished')\n",
    "    \n",
    "    # 10th line\n",
    "    mesh[:, :, 9, 5, 0] = X[:,:,63]; print('10th finished')\n",
    "    \n",
    "    return mesh\n",
    "\n",
    "def create_folder(test_ratio, set_seed, user_independent):\n",
    "    print('creating folders')\n",
    "        \n",
    "    if user_independent:\n",
    "        fold_name = 'user_independent'\n",
    "    else:\n",
    "        fold_name = 'user_dependent'\n",
    "        \n",
    "    DIRNAME = './dataset/splitted_data/'+args.data_type+'/'+fold_name\n",
    "    \n",
    "    if not os.path.exists(os.path.dirname(DIRNAME)):\n",
    "        os.makedirs(os.path.dirname(DIRNAME))\n",
    "        \n",
    "    DIRNAME = DIRNAME + '/test_rate_' + str(test_ratio) + '/seed_' + str(set_seed)+'/'\n",
    "    \n",
    "    if not os.path.exists(os.path.dirname(DIRNAME)):\n",
    "        os.makedirs(os.path.dirname(DIRNAME))\n",
    "        \n",
    "    return DIRNAME\n",
    "\n",
    "\n",
    "def split_data(X, y, p, test_ratio, set_seed, user_independent):\n",
    "    # Shuffle trials\n",
    "    np.random.seed(set_seed)\n",
    "    if user_independent:\n",
    "        trials = len(set([ele[0] for ele in p]))\n",
    "    else:    \n",
    "        trials = X.shape[0]\n",
    "    print('trial', trials)\n",
    "    shuffle_indices = np.random.permutation(trials)\n",
    "    \n",
    "    print('-- shaffleing X, y, p')\n",
    "    if user_independent:\n",
    "        # Create a dict with empty list as default value.\n",
    "        d = defaultdict(list)\n",
    "        # print(y.shape, np.array([ele[0] for ele in y]).shape)\n",
    "        for index, e in enumerate([ele[0] for ele in y]):\n",
    "            # print('index', index, 'e', e)\n",
    "            d[e].append(index)\n",
    "        new_indexes = []\n",
    "        for i in shuffle_indices:\n",
    "            new_indexes += d[i]\n",
    "        X = X[new_indexes]\n",
    "        y = y[new_indexes]\n",
    "        p = p[new_indexes]\n",
    "        train_size = 0\n",
    "        for i in shuffle_indices[:int(trials*(1-test_ratio))]:\n",
    "            train_size += len(d[i])\n",
    "                \n",
    "    else:\n",
    "        X = X[shuffle_indices]\n",
    "        y = y[shuffle_indices]\n",
    "        p = p[shuffle_indices]\n",
    "        # Test set seperation\n",
    "        train_size = int(trials*(1-test_ratio)) \n",
    "    \n",
    "    print('-- split X, y, p in train-test',train_size)\n",
    "    \n",
    "    # X_train, X_test, y_train, y_test, p_train, p_test\n",
    "    return  X[:train_size,:,:], X[train_size:,:,:], y[:train_size,:], y[train_size:,:], p[:train_size,:], p[train_size:,:]\n",
    "               \n",
    "def prepare_data(X, y, p, test_ratio, return_mesh, set_seed, user_independent):\n",
    "    \n",
    "    # y encoding\n",
    "    # oh = OneHotEncoder(categories='auto')\n",
    "    # y = oh.fit_transform(y).toarray()\n",
    "    DIRNAME = create_folder(test_ratio, set_seed, user_independent)\n",
    "    print('Folder: ',DIRNAME)\n",
    "    \n",
    "    print('Split dataset:')\n",
    "    X_train, X_test, y_train, y_test, p_train, p_test = split_data(X, y, p, test_ratio, set_seed, user_independent)\n",
    "                                    \n",
    "    # Z-score Normalization\n",
    "    def scale_data(X):\n",
    "        shape = X.shape\n",
    "        for i in range(shape[0]):\n",
    "            # Standardize a dataset along any axis\n",
    "            # Center to the mean and component wise scale to unit variance.\n",
    "            X[i,:, :] = scale(X[i,:, :])\n",
    "            if i%int(shape[0]//10) == 0:\n",
    "                print('{:.0%} done'.format((i+1)/shape[0]))   \n",
    "        return X\n",
    "    \n",
    "    print('Scaling data')\n",
    "    print('-- X train-test along any axis')\n",
    "    X_train, X_test  = scale_data(X_train), scale_data(X_test)\n",
    "    \n",
    "    if return_mesh:\n",
    "        print('Creating mesh')\n",
    "        print('-- X train-test to mesh')\n",
    "        X_train, X_test = convert_mesh(X_train), convert_mesh(X_test)\n",
    "    \n",
    "    return DIRNAME, X_train, X_test, y_train, y_test, p_train, p_test\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating folders\n",
      "Folder:  ./dataset/splitted_data/user_independent/test_rate_0.2/seed_42/\n",
      "Split dataset:\n",
      "trial 105\n",
      "-- shaffleing X, y, p\n",
      "-- split X, y, p in train-test 377475\n",
      "Scaling data\n",
      "-- X train-test along any axis\n",
      "0% done\n",
      "10% done\n",
      "20% done\n",
      "30% done\n",
      "40% done\n",
      "50% done\n",
      "60% done\n",
      "70% done\n",
      "80% done\n",
      "90% done\n",
      "100% done\n",
      "0% done\n",
      "10% done\n",
      "20% done\n",
      "30% done\n",
      "40% done\n",
      "50% done\n",
      "60% done\n",
      "70% done\n",
      "80% done\n",
      "90% done\n",
      "100% done\n",
      "Creating mesh\n",
      "-- X train-test to mesh\n",
      "1st finished\n",
      "2nd finished\n",
      "3rd finished\n",
      "4th finished\n",
      "5th finished\n",
      "6th finished\n",
      "7th finished\n",
      "8th finished\n",
      "9th finished\n",
      "10th finished\n",
      "1st finished\n",
      "2nd finished\n",
      "3rd finished\n",
      "4th finished\n",
      "5th finished\n",
      "6th finished\n",
      "7th finished\n",
      "8th finished\n",
      "9th finished\n",
      "10th finished\n"
     ]
    }
   ],
   "source": [
    "DIRNAME, X_train, X_test, y_train, y_test, p_train, p_test = \\\n",
    "            prepare_data(X, y, p, test_ratio=0.2, return_mesh=True, set_seed=42, user_independent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (377475, 10, 10, 11, 1) y_train (377475, 1) p_train 105 X_test (240873, 10, 10, 11, 1) y_test (240873, 1) p_test (240873, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X_train', X_train.shape, \\\n",
    "      'y_train', y_train.shape, \\\n",
    "      'p_train', (len(set([p[0] for p in p_train]))), '\\n'\n",
    "      'X_test', X_test.shape,  \\\n",
    "      'y_test', y_test.shape,  \\\n",
    "      'p_test', p_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial 105\n",
      "-- shaffleing X, y, p\n",
      "-- split X, y, p in train-test 377475\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val, p_train, p_val = \\\n",
    "            split_data(X_train, y_train, p_train, test_ratio=0.2, set_seed=42, user_independent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (377475, 10, 10, 11, 1) y_train (377475, 1) p_train (377475, 1) X_test (240873, 10, 10, 11, 1) y_test (240873, 1) p_test (240873, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X_train', X_train.shape, \\\n",
    "      'y_train', y_train.shape, \\\n",
    "      'p_train', p_train.shape, \\\n",
    "      'X_test', X_test.shape,  \\\n",
    "      'y_test', y_test.shape,  \\\n",
    "      'p_test', p_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( [X_train, y_train, p_train] ,open( DIRNAME + \"train.p\", \"wb\" ) , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( [X_val, y_val, p_val]  ,open( DIRNAME + \"val.p\", \"wb\" ) , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( [X_test, y_test, p_test]  ,open( DIRNAME + \"test.p\", \"wb\" ) , protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the EEG recording instrument has 3D locations over the subjects\\` scalp, it is essential for the model to learn from the spatial pattern as well as the temporal pattern. I transformed the data into 2D meshes that represents the locations of the electrodes so that stacked convolutional neural networks can grasp the spatial information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
